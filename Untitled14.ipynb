{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPnwIfUwMrZ6giM6zsOupx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh6103/key-extract/blob/master/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hcMMZxBLoDfp"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "HCuEbtwOoR0E"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "YHa3mrWFoVG-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6jBPQFv6oWRq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xQ9IQoxcoah6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeqevA54o4tt",
        "outputId": "8c0cd675-3b52-49c1-db01-1f895a9dac7d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.25.2)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.9)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "2VLUzLP-odiA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "a7Aoy3bPq61Q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "WtiZ90UHrO_L"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TSAnQUSr3PC",
        "outputId": "8b8f6bb7-ad77-4f30-cd8c-925d48c03ee0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji"
      ],
      "metadata": {
        "id": "fYTHoWQfrZ-P"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm"
      ],
      "metadata": {
        "id": "Dh-RU7odr81n"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "yhSmN9uusE08"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Oe2MZ4QIsILw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "eyWZJTposL5r"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()"
      ],
      "metadata": {
        "id": "ppPhB2E_sP3O"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_chars(text):\n",
        "    eng_alphabets = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\\\n",
        "                     'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "    eng_pattern = r'(?:{})'.format('|'.join([r\"{}\".format(ch) for ch in eng_alphabets]))\n",
        "\n",
        "    with open(\"words_with_2_duplicate_letters.txt\", encoding=\"utf-8\") as text_file:\n",
        "        certain_words = [line.strip() for line in text_file.readlines()]\n",
        "\n",
        "    text = text.lower()\n",
        "    # if url exists in the text\n",
        "    url_pattern = r\"(?:(?:https?):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+\"\n",
        "    if re.findall(url_pattern, text, re.IGNORECASE):\n",
        "        text = re.sub(url_pattern,\"\",text)\n",
        "\n",
        "    # if number exists in the text\n",
        "    # if re.findall(r\"(\\d+)\", text, re.IGNORECASE):\n",
        "    #     text = re.sub(r\"(\\d+)\",\"\",text)\n",
        "\n",
        "    # if hashtag exists in the text\n",
        "    # if re.findall(r\"#(\\w+)\", text):\n",
        "    #     text = re.sub(r\"#(\\w+)\",\"\",text)\n",
        "\n",
        "    # if username exists in the text\n",
        "    if re.findall(r\"@(\\w+[.]*\\b)\", text):\n",
        "        text = re.sub(r\"@(\\w+[.]*\\b)\",\"\",text)\n",
        "\n",
        "    # if number length is more than 3 numbers\n",
        "    text = re.sub(r'\\d{3,}', '', text)\n",
        "\n",
        "    # remove english characters\n",
        "    text = re.sub(eng_pattern, '', text)\n",
        "\n",
        "    # if text contains @\n",
        "    # text = re.sub(r'\\s@\\s', '', text)\n",
        "\n",
        "    # if text contains extra character\n",
        "    # remove all punctuation in fasttext or glove in ai_model\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    text = regex.sub(' ', text)\n",
        "\n",
        "    text = unifing_alphabets(text)\n",
        "\n",
        "    # if text contains duplicate letters\n",
        "    # with open(os.getcwd()+\"/resources/words_with_2_duplicate_letters.txt\", encoding=\"utf-8\") as tfile:\n",
        "    #     certain_words = [line.strip() for line in tfile.readlines()]\n",
        "    duplicate_letter_pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    text_with_pattern2 = duplicate_letter_pattern.sub(r\"\\1\\1\", text)\n",
        "    differences2 = [word for word in text_with_pattern2.split(\" \") if word not in text.split(\" \")]\n",
        "    if differences2:\n",
        "        curse_pattern = r'\\b(?:{})\\b'.format('|'.join(certain_words))\n",
        "        if re.findall(curse_pattern, \" \".join(differences2)):\n",
        "            text = text_with_pattern2\n",
        "        else:\n",
        "            text = duplicate_letter_pattern.sub(r\"\\1\", text)\n",
        "\n",
        "    # if text contains #\n",
        "    # text = re.sub(r'\\s#\\s', '', text)\n",
        "\n",
        "    text = (\" \").join([word.strip() for word in text.split()])\n",
        "    return text"
      ],
      "metadata": {
        "id": "zLSbxEYhsYEp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji_space(text):\n",
        "    regex = re.compile(r'\\d+(.*?)(?:\\u263a|\\U0001f645)')\n",
        "    all_emoji = regex.findall(text)\n",
        "\n",
        "    edited_text = []\n",
        "    for c in text:\n",
        "        if c in emoji.UNICODE_EMOJI:\n",
        "            edited_text.append(\" \"+c+\" \")\n",
        "        else:\n",
        "            edited_text.append(c)\n",
        "\n",
        "    edited_text = (\"\").join(edited_text)\n",
        "    return(edited_text)"
      ],
      "metadata": {
        "id": "r26wJ4bW19wO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unifing_alphabets(text):\n",
        "    text = re.sub(r'اَ|اِ|اُ|اٌ|اٍ|اً|أ|إ', 'ا', text)\n",
        "    text = re.sub(r'بَ|بِ|بُ|بّ', 'ب', text)\n",
        "    text = re.sub(r'پَ|پِ|پُ', 'پ', text)\n",
        "    text = re.sub(r'تَ|تِ|تُ|تّ', 'ت', text)\n",
        "    text = re.sub(r'ثَ|ثِ|ثُ', 'ث', text)\n",
        "    text = re.sub(r'جَ|جِ|جُ|جّ', 'ج', text)\n",
        "    text = re.sub(r'چَ|چِ|چُ|چّ', 'چ', text)\n",
        "    text = re.sub(r'حَ|حِ|حُ', 'ح', text)\n",
        "    text = re.sub(r'خَ|خِ|خُ', 'خ', text)\n",
        "    text = re.sub(r'دَ|دِ|دُ|دّ', 'د', text)\n",
        "    text = re.sub(r'ذَ|‌ذِ|ذُ', 'ذ', text)\n",
        "    text = re.sub(r'رَ|رِ|رُ|رّ', 'ر', text)\n",
        "    text = re.sub(r'زَ|زِ|زُ|زّ', 'ز', text)\n",
        "    text = re.sub(r'ژَ|ژِ|ژُ', 'ژ', text)\n",
        "    text = re.sub(r'سَ|سِ|سُ|سّ', 'س', text)\n",
        "    text = re.sub(r'شَ|شِ|شُ', 'ش', text)\n",
        "    text = re.sub(r'صَ|صِ|صُ', 'ص', text)\n",
        "    text = re.sub(r'ضَ|ضِ|ضُ', 'ض', text)\n",
        "    text = re.sub(r'طَ|طِ|طُ', 'ط', text)\n",
        "    text = re.sub(r'ظَ|ظِ|ظُ', 'ظ', text)\n",
        "    text = re.sub(r'عَ|عِ|عُ', 'ع', text)\n",
        "    text = re.sub(r'غَ|غِ|غُ', 'غ', text)\n",
        "    text = re.sub(r'فَ|فِ|فُ|فّ', 'ف', text)\n",
        "    text = re.sub(r'قَ|قِ|قُ|قّ', 'ق', text)\n",
        "    text = re.sub(r'کَ|کِ|کُ|ك|كَ|كِ|كُ|کّ|كّ', 'ک', text)\n",
        "    text = re.sub(r'گَ|گِ|گُ', 'گ', text)\n",
        "    text = re.sub(r'لَ|لِ|لُ|لّ', 'ل', text)\n",
        "    text = re.sub(r'مَ|مِ|مُ', 'م', text)\n",
        "    text = re.sub(r'نَ|نِ|نُ|نّ', 'ن', text)\n",
        "    text = re.sub(r'وَ|وِ|وُ|ؤ|ؤَ|ؤُ|ؤِ|وّ|ؤّ', 'و', text)\n",
        "    text = re.sub(r'هَ|هِ|هُ|ة', 'ه', text)\n",
        "    text = re.sub(r'یَ|یِ|یُ|ي|يَ|يِ|يُ|يّ|یّ', 'ی', text)\n",
        "    text = re.sub(r'ئَ|ئِ|ئُ|ئّ', 'ئ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ttr8bTke2ERa"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path =\"sample_data/test.txt\"\n",
        "\n",
        "test_dataset_path= \"sample_data/test.txt\"\n",
        "#\n",
        "# model = fasttext.train_supervised(train_dataset_path,\n",
        "#                                  dim=100,\n",
        "#                                  lr=2.0, #[0.1 - 1.0]\n",
        "#                                  loss=\"hs\",\n",
        "#                                  wordNgrams=7, #[1 - 5]\n",
        "#                                  ws=3,\n",
        "#                                  epoch=100, #[5 - 50]\n",
        "#                                  minn=3,#3\n",
        "#                                  maxn=6,#6\n",
        "#                                  lrUpdateRate=100,\n",
        "#                                  neg=10,\n",
        "#                                  t=0.0001\n",
        "#                                  )\n",
        "\n",
        "fasttext_model = fasttext.train_supervised(train_dataset_path,\n",
        "                                 dim=100,\n",
        "                                 lr=0.1, #[0.1 - 1.0]\n",
        "                                 loss=\"hs\",\n",
        "                                minCount = 1,\n",
        "                                 wordNgrams=1, #[1 - 5]\n",
        "                                 ws=5,\n",
        "                                 epoch=100, #[5 - 50]\n",
        "                                 minn=2,#3\n",
        "                                 maxn=5,#6\n",
        "                                 lrUpdateRate=100,\n",
        "                                 neg=5,\n",
        "                                 t=0.0001,\n",
        "                                 verbose=2,\n",
        "                                 bucket=2000000\n",
        "\n",
        "                                 )"
      ],
      "metadata": {
        "id": "5GLTNS_o2KZq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model.quantize(input=train_dataset_path,\n",
        "               qnorm=True,\n",
        "               retrain=True,\n",
        "               epoch=1,\n",
        "               cutoff=100000\n",
        "               # dsub=,\n",
        "               # qout=\n",
        "               )"
      ],
      "metadata": {
        "id": "HDojh5aPJgzd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model_path = \"sample_data/dgkaladata.ftz\"\n",
        "fasttext_model.save_model(quantized_model_path)\n",
        "\n",
        "print(\"test: \", fasttext_model.test(test_dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-q8BVrTKHNo",
        "outputId": "3864d75c-a48d-4cf0-d68d-40375c16ed38"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test:  (3261, 0.8761116222017786, 0.8761116222017786)\n"
          ]
        }
      ]
    }
  ]
}